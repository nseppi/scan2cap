\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{booktabs,tabularx,enumitem,ragged2e}
\usepackage{subcaption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Project Report: Scan2Cap}

\author{Felix Wimbauer\\
Technical University of Munich\\
{\tt\small felix.wimbauer@tum.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Nicolas Seppich\\
Technical University of Munich\\
{\tt\small nicolas.seppich@tum.De}
\and
\small
Supervisor: Dave Zhenyu Chen\\
\small
Technical University of Munich\\
{\tt\small zhenyu.chen@tum.de }
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In this work, we investigate the task of generating a description for a target object in context to its environment in the 3D domain. To this end, we propose a pipeline which combines concepts from 3D object detection and visual attention-based captioning. The proposed pipeline first uses VoteNet to extract feature vectors of the scene. It then combines this information with the features of the object of interest, which are extracted by PointNet++, and feeds this data into an LSTM captioning mechanism, that generates a caption of the object in context of the scene.
   
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Problem Statement \& Motivation}

Extracting a detailed and semantic correct understanding of the layout of a 3D scene is crucial for many tasks, e.g. in robotics for navigation and interaction with objects. This includes relating the 3D positions of the objects and their spatial extent, so that a semantically correct description of the objects' environment is generated.
However, to the best of our knowledge, there has been no work so far to generate a description of an object representation in point clouds or 3D data.

Therefore, we are interested in implementing a pipeline to obtain a description for a given object in a 3D scan, using state-of-the-art point cloud feature extractors, object detectors, and a captioning mechanism to generate a semantic description for a given object in the 3D scene. This allows the object to be placed in a global semantic context within its environment. Furthermore, our project lays down the groundwork for dense captioning, in which you obtain a semantic representation of the scene through computing a caption for all objects in that scene.
 
%-------------------------------------------------------------------------
\section{Related Work}
Our work is based on the ScanRefer dataset \cite{chen2019scanrefer}. This dataset consists of 1513 RGB-D scans of ScanNet \cite{dai2017scannet} and contains approximately 5 unique object descriptions for each object in each scene. The work of \cite{chen2019scanrefer} will also be used as guideline in this project.

The extraction of features on point clouds is presented by \cite{qi2017pointnet++}, who apply the feature extraction directly on the point cloud on a hierarchical level, allowing the extraction of local features in a global context. 
The task of object detection on point clouds is studied by \cite{qi2019deep}. 

Methods for image captioning using visual attention are described by \cite{xu2015show}, \cite{lu2017knowing} and \cite{anderson2018bottom}.
These methods have in common, that they generate a caption for the entire image.

%-------------------------------------------------------------------------
\section{Architecture}

Given a point cloud $\mathit{p \in R^{N\times(d+C)}}$ and an object in that scene, which is described by a target bounding box $b_{target}\in R^6$, our goal is to generate a meaningful caption for the object embedded in the context of the scene. To this end, we used three different pipelines, which are described in the following.

\subsection{Baseline}

To extract information from the point cloud, we use a PointNet++ \cite{qi2017pointnet++} model. To give the network information about which object we are interested in, we add a new feature channel to each point that masks all points that lay within the bounding box of the object. To ensure that we receive meaningful features, we use weights pretrained for classification of the masked object. The idea behind this is that classification will not only use information from the masked objects, but also global features, for example from close-by objects. 

To generate the caption, we use a classical LSTM with an appended fully-connected layer. The fully-connected layer acts as a word classifier and maps the hidden state $h_{t}$ of the LSTM to our vocabulary. As input, the LSTM receives the feature vector extracted from the point cloud and the word embedding vector of the previously generated word. The word embedding is taken from a pre-computed GloVe \cite{pennington2014glove} word embedding matrix. This structure of the iterative caption generation is similar to \cite{xu2015show}.

\subsection{Better Feature Extraction with VoteNet}

Because PointNet++ is pretrained to classify the object of interest there is no guarantee that the feature vector will give high-quality information about the global context, thus limiting the baseline approach. In the second architecture iteration, we therefore employ a VoteNet \cite{qi2019deep} network, which computes a fixed number of object proposals and according feature vectors for our point cloud. Those feature vectors nicely describe the context of our scan and help the network to understand the surroundings of the object we want to describe. Because the number of proposals may vary and the proposals are not in a fixed order, we average pool them to obtain a concise representation of the information. This pooled feature vector is finally concatenated with the feature vector from PointNet++ and the embedding vector of the previously predicted word to then be passed into the LSTM.

\subsection{Better Captioning with Attention}

Average pooling the feature vectors from VoteNet is not ideal as often only a small number of the object proposals is relevant for the final caption. Therefore, in the third model iteration we replace the average pooling step from before with an attention mechanism, as it is described in \cite{xu2015show}. The attention mechanism receives the hidden state of the captioning LSTM from the previous iteration and the feature vectors of the object proposals. It then uses a series of fully-connected layers to predict relevancy scores for the different object proposals that are turned into probabilities using the softmax function. Instead of average pooling, we can now multiply all feature vectors with their respective probability and sum over all of them. This approach allows our model to select the most relevant object for each token in the caption and make word predictions that better match the the context of the scene.

\autoref{fig:architectures} summarizes the project architectures.

\section{Experiments \& Results}

To allow a quantitative comparison between the proposed architectures, we have constructed the same training pipeline for the three models. As mentioned above, the models receive the point cloud of a scene, the ground truth bounding box of the target object and the tokenized description as input.
Similar to \cite{xu2015show}, all architectures make use of teacher-forcing during training: instead of inserting the previous predicted word, the LSTM receives the previous ground truth word as input for captioning along with the coded feature vectors of the scene.  This allows for faster convergence by minimizing drift between the ground truth token and predicted word in training time. In evaluation time, the network takes the previous predicted word as input. 

As metrics for performance evaluation we rely on BLEU \cite{Papineni2002BleuAM}, ROUGE-L \cite{Lin2004ROUGEAP}, METEOR \cite{Denkowski2014MeteorUL} and CIDEr \cite{DBLP:journals/corr/VedantamZP14a}.  In this paper we focus on the BLEU score to determine the best models as in \cite{xu2015show}. 

For our models based on VoteNet we filter out all objects that have an object frequency value below 0.75. We also consider only the 8 closest objects, because these thresholds lead to better results representing the local scene context.   

\subsection{Quantitative Improvements}
In this experiment we want to investigate the influence of the architectural improvements we propose. All models include the same pre-trained masked PointNet++ feature extractor. 
\autoref{tab:quantitative_results} shows the quantitative results. 

As a first result, we can conclude that the baseline model achieves reasonably good results that are not far from the improved architectures. Comparing the BLEU-4 score to the results of the COCO Image Captioning Challenge 2015 "quote", our models outperform the best-performing architectures. Of course, the question always arises whether our task and the tasks of COCO Image Captioning are comparable. We argue that our vocabulary and gt sentence structure is much simpler and therefore leads to better results.

Secondly, the results show that improving the feature extractor (improved architecture) and improving the captioning meachnism (attention) lead to higher scores. The best results are obtained by the attention model, except for the BLEU-4 score where the improved architecture is slightly better.                

\subsection{Qualitative Analysis}
In this experiment we want to analyze the quality of the predicted descriptions. For this purpose, we compare two exemplary ground truth and predicted description pairs shown in Figure X. 
The first pair shows the comparison between the ground truth and the predictions of our three architectures. The result shows that the three models predict a correct description that puts the object into the context of its environment. There are slight differences in the context the models make use of, especially for the attention model. This indicates that the attention mechanism is capable of producing a description that is rather unusual. The baseline and improved architecture were able to predict a chair near a table, which is a fairly common scenario. 
The second pair, which illustrates a long ground truth description and the prediction of the attention model, shows the accuracy of the prediction, even if it is quite short. 

For all predicted descriptions we can observe a simple structure with two sentences. As an explanation we claim that the statistics of the ScanRefer data set influences this behavior. Therefore we have analyzed the number of tokens per description and the number of records in each description. Figures Z and Y support this assertion by showing the hsitograms of the number of tokens and record lengths for our predictions and for the ScanRefer data set, respectively.


\subsection{Inference without GT}

\section{Coclusion}

\begin{figure*}
	\begin{subfigure}[c]{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/arch_baseline.pdf}
		\caption{Baseline model}
		\label{fig:baseline}
	\end{subfigure}
	\begin{subfigure}[c]{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/arch_votenet.pdf}
		\caption{Better feature extraction with VoteNet}
		\label{fig:votenet}
	\end{subfigure}
	\begin{subfigure}[c]{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/arch_attention.pdf}
		\caption{Better captioning with attention}
		\label{fig:attention}
	\end{subfigure}
	\caption{Model architectures for project}
	\label{fig:architectures}
\end{figure*}

\begin{table*}
	\centering
	\begin{tabular}{l|cccc}
		\textbf{Model} & \textbf{BLEU-4} & \textbf{ROUGLE-L} & \textbf{METEOR} & \textbf{CIDEr}\\
		\hline
		Baseline & 0.434 & 0.569 & 0.617 & 0.692 \\
		With VoteNet & \textbf{0.447} & 0.580 & 0.625 & 0.757 \\
		With Attention & 0.447 & \textbf{0.584} & \textbf{0.626} & \textbf{0.792}  
	\end{tabular}
	\caption{Quantitative results}
	\label{tab:quantitative_results}
\end{table*}

\begin{figure*}
	\centering
	\begin{subfigure}[c]{.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gt_scene0221_00_3_0_att_02.png}
		\caption{
			\textbf{GT}: there is a black chair. it is next to a cabinet on the side of the room .\\
			\textbf{BL}: this is a black chair . it is to the left of the table .\\
			\textbf{VN}: this is a black chair . it is to the left of the desk .\\
			\textbf{Att}: this is a black chair . it is at the end of the \underline{bed} .}
		\label{fig:example_1}
	\end{subfigure}
	\begin{subfigure}[c]{.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/gt_scene0164_00_2_0_att_06.png}
		\caption{
			\textbf{GT}: the open kitchen cabinet is directly above the sink and the water container. the open kitchen cabinet is a brown box with one side hanging out .\\
			\textbf{Att}: this is a white kitchen cabinet . it is above the \underline{sink} .\\}
		\label{fig:example_2}
	\end{subfigure}
	\vspace{.5\baselineskip}\\
	{\footnotesize
		\textbf{Legend}: \textbf{GT}-Groundtruth, \textbf{BL}-Baseline, \textbf{VN}-With VoteNet, \textbf{Att}-With Attention\\
		\textbf{Bounding boxes}: \textbf{\color{green} Green}-Target object, \textbf{{\color{red}Red}-{\color{blue}Blue}}-Lowest to highest attention for marked token
	}
	\caption{Examples from the ScanRefer validation set}
	\label{fig:examples}
	
\end{figure*}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{report}
}

\end{document}
